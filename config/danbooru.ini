; multiple values can be stored in a file and point to the file by doing:
; file<{encoding}><{separator}>: {path}
; for example: file<utf-8><\n>: myLeastFavoriteArtists.txt
; will read myLeastFavoriteArtists.txt with utf-8 encoding and split the content line by line (\n)
; to not split the file content using separator use None in separator field. ex: file<utf-8><None>: asdf.txt

; Use Empty string or value "none" or "ignore" to bypass the check for the requirement if it requires a boolean (true, false)

; NOTE:: ALL NUMBER VALUES ARE NOT INCLUSIVE.
; FOR EXAMPLE: Min Score of 5 Will only scrape posts with score higher than five
; such as 6, 7, 8, ... but except for 5

[REQUIREMENTS]
TAGS =
; {string(comma-separated)}
; list of tags to put in query

; POOR PEOPLE'S MULTI QUERY SEARCH :O
TAGS_EXTRA =
; {string(comma-separated)}
; list of tags that is not present in query but still needs be appear on image
; this is helpful when you wanna search multiple (more than) tags and you don't have gold

TAGS_EXCLUDE =
; {string(comma-separated)}
; list of tags to exclude in query, don't put the minus (-) sign in the front

TAGS_EXCLUDE_EXTRA =
; {string(comma-separated)}
; list of tags that is not present in query but still needs be appear on image

MAX_TAG_QUERY = 2
; {int}
; The max number of different things you can search in the query. default 2 for anon users
; you can have more as gold user, but i have yet implemented the ability to log in

RATING = r
; {string(pre-defined)}
; the rating that image has to be in order to be scraped
; r - explicit; q - questionable; s - safe

; TODO: Optimize this, if it's true, put it in to the query.
RATING_CHECK_STRICT = false
; {bool}
; CANNOT BE IGNORE OR NONE
; if the rating must be the same instead of inclusive
; for example: with this setting set to true and rating set to r
; only images rated explicit will be scraped, but with out this setting,
; both be safe, questionable and explicit images will be scraped

CHILDREN = ignore
; {bool}
; This setting can be ignored
; true if the post needs to have a children in order to be scraped
; false if the post can't have a children
; use ignore or none to disregard this field

EXTENSION =
; {string(comma-separated)}
; why tf would you use this

EXTENSION_EXCLUDE =
; {string(comma-separated)}
; why tf would you use this

; UPLOADER_INCLUDE =

; UPLOADER_EXCLUDE =

MIN_SCORE = 20
; {int}
; the score an image needs to have to satisfy the requirement.

MIN_FAV_COUNT = 20
; {int}
; the minimal number of favorites an image needs to have to satisfy the requirement.

MAX_WIDTH = 999999999
; {int}
; The max width the image (in pixels) can be before it's considered unsatisfactory
; Put some impossible values such as 99999999 to have any max width
; same for the height

MAX_HEIGHT = 999999999
; {int}
; The max height the image (in pixels) can be before it's considered unsatisfactory

MIN_WIDTH = -1
; {int}
; the minimal width the image (in pixels) needs to have before it's considered okay
; use any number lower or equal to 0 to accept all width

MIN_HEIGHT = -1
; {int}
; the minimal height the image (in pixels) needs to have before it's considered okay

; TIME_START =
; NOT YET IMPLEMENTED

; TIME_END =
; NOT YET IMPLEMENTED

SOURCE_ORIGIN =
; {string(comma-separated)}
; list of source website the image must be from
; domain name of the source without the TLD (top level domain), Only the SLD (second level domain)
; for example: www.pixiv.net -> pixiv or www.twitter.com -> twitter

SOURCE_ORIGIN_EXCLUDE =
; {string(comma-separated)}
; list of source the image can't be from

[RANGE]
START_PAGE = 1

END_PAGE = 15

[Output]
SAVE_PATH = .
; {string(path like)}
; Save scraped image to this READABLE/WRITEABLE directory

MASTER_DIRECTORY_NAME_STRING = danbooru {start_page}-{end_page} {rating}
; {string}
; Any settings that are present in this file, but all lower cased
; for example COLLECT_DATA_ONLY will become {collect_data_only},
; DO NOT PUT SPECIAL CHARACTERS, CANNOT BE EMPTY

FILENAME_STRING = {image_id}-{image_score}-{image_fav_count}-{img_index}.{image_extension}
; {string}
; what to name the scraped images, CANNOT BE EMPTY
; Available Fields everything below with one extra
;                   {image_id}
;                   {image_tags}  // Do not use, very very long string
;                   {image_rating}
;                   {image_width}
;                   {image_height}
;                   {image_flags}  // tbh i have no idea what do the flags do
;                   {image_has_children}
;                   {image_score}
;                   {image_fav_count}
;                   {image_extension}
;                   {image_source}
;                   {img_index} // THIS HAVE TO BE INCLUDED IF YOU DON"T WANT IMAGES TO BE OVERWRITTEN

CSV_ENTRY_STRING = {image_id}, {image_tags}, {image_score}, {image_fav_count}, {image_source}, {image_width}, {image_height}
; {string}
; (allows same fields as FILENAME_STRING)
; columns to write into the csv file

[Advanced]
MAX_CONCURRENT_THREAD = 3
; {int}
; Max thread that will be used to scrape the images, each page will have their individual threads
; More than 5 threads probably will get your ip temp banned

THREAD_NAME_PREFIX = danbooru

DOWNLOAD_DELAY = 1.4
; {float}
; Wait this seconds before starting downloading the image

DELAY_START = 0

READ_IMG_INFO_DELAY = 0.2
; {float}
; Wait this seconds before reading the next images information

COLLECT_DATA_ONLY = false
; {boolean}
; True if the scraper will only write image's data without downloading the image
; useful if you want to quickly view the top ranking image w/o premium

USER_AGENT = Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4018.0 Safari/537.36
; Use this user-agent to download stuff

FLUSH_CSV_IMMINENTLY = false
; {bool}
; True to Imminently save csv file using .flush(), reduces performances
; False to save the written file only when the scraping is completely finished

USE_SUBMISSION_SPECIFIC_DIRECTORY = false
; {bool}
; store the downloaded image in it's own directory instead of storing it in the page directory with other images
; useless for danbooru because it only have one image for each post

[Cleanup]
MERGE_FILE = True
; {boolean}
; merge all scraped images in separated page folders in to one

MERGE_FILE_KEEP_SEPARATE = False
; {boolean}
; Not yet implemented (i think?)


[Logger]
LOGGER_FILE = WARNING
; {predefined-string}
LOGGER_STDOUT = DEBUG
; {predefined-string}
; the level for the logger that output to file, if you are an user recommend use INFO
; Available logging levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
;                           Sorted in order from the MOST info to LEAST

LOGGER_NAME = danbooru
