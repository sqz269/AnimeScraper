[Requirements]
TAGS =
; {string(space separated)}
; Tags that will be present in the search query

SEARCH_MODE =
; {predefined-string}
; Search Mode partial_match_for_tags, exact_match_for_tags, title_and_caption

TAGS_EXCLUDE =
; {string(comma separated)}
; Won't scrape illustrations with this tag
; Splitting tag by ,

TAGS_BYPASS =
; {string(space separated)}
; Always scrape illustrations with this tag

; TAGS_BYPASS_FULL =

ILLUST_TYPE =
; {predefined-string}
; Only scrape a very certain type of illustrations
; available types: illust, video, manga
; Leave blank to accept all types

BOOKMARK_MIN =
; {int}
; minimal bookmarks in order for the illustration to be scraped
; Overridden by every other options

VIEW_MIN =
; {int}
; minimal views in order for the illustration to be scraped

VIEW_BOOKMARK_RATIO =
; {float}
; The ratio of views per bookmark in order for the illustration to be scraped
; ratio is calculated using floor(view count / bookmark count)

AVG_BOOKMARK_PER_DAY =
; {float}
; Minimal book marks per 24 hours

VIEW_BOOKMARK_RATIO_BYPASS =
; {float}
; Bypass view/bookmark ratio and avg bookmark per day if total bookmarks reach this amount

R18 =
; {boolean}
; True if you want to include R18 Images else False

R18_CHECK_STRICT =
; {boolean}
; true if you ONLY want the rating specified in R18 options to be scraped.
; Only have effects if R18 is enabled, then only R18 images will be scraped

[Range]
START_PAGE =
; {int}
END_PAGE =
; {int}
; START 1 END 50, Check first 50 pages, or 50 * 30 = 1500 images

SORTED_BY =
; {predefined-string}
; Sorting image using this priory
; available fields are date_desc, date_asc

TIME_SPAN = False
; {predefined-string}
; Last_day, Last_week, Last_month, False

IMAGE_SIZE = original

[Authorization]
USERNAME =
PASSWORD =

[Output]
SAVE_PATH =
; {string(path like)}
; Save scraped image to this READABLE/WRITEABLE directory

MASTER_DIRECTORY_NAME_STRING =
; {string}
; Any settings that are present in this file, but all lower cased
; for example COLLECT_DATA_ONLY will become {collect_data_only},
; DO NOT PUT SPECIAL CHARACTERS

FILENAME_STRING =
; {string}
; Available Fields everything below with one extra
;                   {img_index} // THIS HAVE TO BE INCLUDED IF YOU DON"T WANT IMAGES TO BE OVERWRITTEN

CSV_ENTRY_STRING =
; {string}
; Available fields: {image_id},      // IllustId
;                   {title},   // Title of the illust
;                   {type},    // Submission type
;                   {tags},    // Tags will be look like [tags 1, tags2, tags3] in one field
;                   {user_id},
;                   {user_name}
;                   {width},
;                   {height},
;                   {total_bookmarks}
;                   {total_view}
;                   {create_date}

[Advanced]
MAX_CONCURRENT_THREAD = 1
; {int}
; Max thread that will be used to scrape the images, each page will have their individual threads

DOWNLOAD_DELAY = 1.4
; {float}
; Wait this seconds before starting downloading the image

DELAY_START = 0

READ_IMG_INFO_DELAY = 0.2
; {float}
; Wait this seconds before reading the next images information

COLLECT_DATA_ONLY = false
; {boolean}
; True if the scraper will only write image's data without downloading the image
; useful if you want to quickly view the top raking image w/o premium

USER_AGENT = Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4018.0 Safari/537.36
; Use this user-agent to download stuff

FLUSH_CSV_IMMINENTLY = false
; {bool}
; True to Imminently save csv file using .flush(), reduces performances
; False to save the written file only when the scraping is completely finished

THREAD_NAME_PREFIX = pixivFast
;

USE_SUBMISSION_SPECIFIC_DIRECTORY =
; {bool}
;

[Cleanup]
MERGE_FILE = True
; {boolean}

MERGE_FILE_KEEP_SEPARATE = False
; {boolean}


[Logger]
LOGGER_FILE = WARNING
; {predefined-string}
LOGGER_STDOUT = DEBUG
; {predefined-string}

LOGGER_NAME = default
; Available logging levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
;                           Sorted in order from the MOST info to LEAST
